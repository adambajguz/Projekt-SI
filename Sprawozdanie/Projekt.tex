\documentclass[10pt,a4paper]{article}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{polski}
\usepackage[left=1in,right=1in,top=1in,bottom=1in]{geometry}
\usepackage{lmodern}

\usepackage{graphicx}

\usepackage{booktabs}

\usepackage{enumitem}

\usepackage[hyphens]{url}
\usepackage[hidelinks]{hyperref}
\hypersetup{breaklinks=true}

\graphicspath{ {./img/} }

\bibliographystyle{elsarticle-num}

\renewcommand{\figurename}{Rys.}
\renewcommand{\tablename}{Tab.}

\begin{document}
\begin{titlepage}
	\centering
	\includegraphics[width=10cm]{PB_logo}\par\vspace{1cm}

	{\LARGE\bfseries Sztuczna inteligencja \par}
	\vspace{0.5cm}
	
	{\LARGE\bfseries Sprawozdanie z projektu końcowego\par}
	\vspace{1.5cm}
	
	{\Large Temat: Prezentacja możliwości biblioteki scikit-learn w projektach informatycznych, wymagających użycia drzew decyzyjnych.\par}
	\vspace{3cm}
	
	\raggedright
	
	
 	{\large Wykonujący projekt: \textbf{Adam Bajguz}\\
 			\makebox[0pt][l]{}\phantom{Wykonujący projekt: }\textbf{Magdalena Kalisz}\par}
 	\vspace{1cm}
 	
 	{\large Studia dzienne \par
	Kierunek: Informatyka \par
	Semestr: IV \hspace{2.5cm} Grupa zajęciowa: \textbf{PS1} \par
	Prowadzący ćwiczenie: \textbf{mgr inż. Dariusz Jankowski} \par}
 	
	\vfill
		
	\begin{tabular}{c}
	\today\\
	Data wykonania projektu
	\end{tabular}
	
	\begin{flushright}
	\begin{tabular}{c}
	..............................................\\
	Data i podpis prowadzącego
	\end{tabular}
	\end{flushright}

\end{titlepage}

\tableofcontents
\newpage

\section{Wstęp}
Drzewa decyzyjne to nieparametryczna nadzorowana metoda uczenia, która może być stosowana zarówno do klasyfikacji, jak i regresji. Pierwszy artykuł, który przedstawia podejście do klasyfikacji znane z drzew decyzyjn, pochodzi z 1959 roku \cite{Belson1959}. Z kolei pierwszy algorytm drzew regresyjnych został obublikowany w roku 1963 \cite{Morgan1963}.\par
\vskip 0.2in
Drzewa decyzyjne są jednym z najczęściej wykorzystywanych narzędzi do klasyfikacji danych i prognozowania z uwagi na fakt, że wiedza odkryta przez drzewo decyzyjne jest zilustrowana w hierarchicznej strukturze. Technika drzew decyzyjnych, czy też klasyfikacyjnych, pozwala m.in. na \cite{MazumdarWWW, Quinlan1986}:
\begin{itemize}
	\item wyznaczenie zasad decyzyjnych opisujących reguły przypisywania obiektów do wyróżnionych klas (zasady odwołują się do wartości atrybutów opisujących obiekty),
	\item analizę zbioru obiektów opisywanych przez przyjęty zestaw atrybutów, której celem jest doskonalenie podziału obiektów na jednorodne klasy.
\end{itemize}

\subsection{Drzewa decyzyjne w teorii decyzji i uczeniu maszynowym}
Drzewa decyzyjne to również graficzny sposób wspierania procesu decyzyjnego. Drzewo stosowane jest w teorii decyzji i ma szereg zastosowań. Może zarówno rozwiązać problem decyzyjny, jak i stworzyć plan. Metoda drzew decyzyjnych sprawdza się również w momencie rozwiązywania problemów decyzyjnych z wieloma rozgałęziającymi się wariantami oraz podejmowania decyzji w warunkach ryzyka. Drzewa znalazły zastosowanie w takich dziedzinach jak botanika i medycyna, a nawet ekonomia, gdyż są w stanie ułatwiać i usprawniać komputerowe wspomaganie procesu podejmowania decyzji \cite{Morgan1963, Quinlan1990}.

\subsection{Rodzaje drzew decyzyjnych}
Rodzaj drzewa decyzyjnego zależy od typu zmiennej docelowej:
\begin{itemize}
	\item drzewo decyzyjne zmiennych jakościowych (drzewo decyzyjne o zmiennej kategorialnej) - drzewo decyzyjne, które ma kategoryczną zmienną docelową,
	\item drzewo decyzyjne zmiennych ilościowych (ciągłych) - drzewo decyzyjne ma ciągłą (ilościową / numeryczną) zmienną docelową.
\end{itemize}\par
\vskip 0.2in

Zaletą drzew decyzyjnych jest to, że mogą modelować dowolny typ funkcji do klasyfikacji lub regresji. Do rozwiązywania problemów zarówno regresyjnych, jaki i klasyfikacyjnych służy metoda CART (rodział \ref{sub:CART}). Wyróżnia się dwa typy drzew decyzyjnych \cite{Breiman1984}:

% \begin{itemize}[label=$\cdot$]
\begin{itemize}
	\item drzewa klasyfikacyjne - służą porządkowaniu klas, charakteryzuje je kategoryczna zmienna zależna, której wartość (czyli przynależność przypadku do klasy, grupy) chcemy poznać na podstawie znajomości wartości jednej lub większej liczby predykcyjnych zmiennych ciągłych oraz, ewentualnie zmiennych kategorialnych;
	\item drzewa regresyjne - służą do przewidywania wartości zmiennej ciągłej, na podstawie znajomości wartości jednej lub większej liczby predykcyjnych zmiennych ciągłych.
\end{itemize}

\section{Budowa drzew decyzyjnych}
Drzewo decyzyjne buduje modele klasyfikacji lub regresji w postaci struktury drzewa, rozkłada zbiór danych na coraz mniejsze podzbiory. Oznacza to, że drzewem decyzyjnym jest graf-drzewo, które składa się z korzenia, węzłów, krawędzi oraz liści. Liście to węzły, z których nie wychodzą już żadne krawędzie. Korzeń drzewa tworzony jest przez wybrany atrybut natomiast poszczególne gałęzie reprezentują wartości tego atrybutu (Rys.~\ref{fig:structure}). \par
\vskip 0.2in
Drzewa decyzyjne charakteryzują się strukturą hierarchiczną (Rys.~\ref{fig:structure}). Oznacza to, że w kolejnych krokach zbiór obiektów jest dzielony, poprzez odpowiedzi na pytania o wartości wybranych cech lub ich kombinacji liniowych. W algorytmach konstrukcji drzew jednym z kluczowych elementów jest wybór kolejności cech, według których, na poszczególnych etapach, będzie dokonywany podział zbioru obiektów. Technika drzew decyzyjnych to uzupełnienie metod klasycznych. Przykładem może tu być analiza dyskryminacyjna. Hierarchiczność podejmowania decyzji jest cechą, która wyróżnia drzewo decyzyjne od innych metod \cite{Gatnar1998, AnalyticsWWW}.

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.9\textwidth]{structure}
	\caption{Budowa (struktura) drzewa decyzyjnego}
	\label{fig:structure}
\end{figure}


\section{Idea drzew decyzji}
Idea drzew opiera się na rekursywnym podziale danych na coraz to mniejsze sterty w celu jak najlepszego dopasowania. Drzewa decyzyjne kodują zestaw reguł if-else, które mogą być używane do przewidywania zmiennej docelowej danych funkcji danych. Reguły if-else są tworzone przy użyciu zestawu danych treningowych w celu zaspokojenia jak największej liczby instancji danych treningowych. Początkowo próbka (węzeł macierzysty, korzeń) dzielona jest na dwa lub więcej podzbiorów (węzły potomne). Natomiast węzeł optymalny wyszukuje się na podstawie wszystkich punktów węzłowych dla każdej zmiennej. Następnie proces jest powtarzany dla każdego węzła potomnego, a te podczas dzielenia traktowane są jak węzły macierzyste. Węzeł, którego nie można już podzielić nazywamy liściem, bądź węzłem końcowym, a liczbę liści - wielkością drzewa \cite{MazumdarWWW, Quinlan1986, Breiman1984, Gatnar1998, Lula2007}.

\section{Cechy drzew decyzyjnych}
Drzewa decyzyjne i uczenie się drzewa decyzyjnego razem stanowią prosty i szybki sposób uczenia się funkcji, która mapuje dane x na wyniki y, gdzie x może być mieszanką zmiennych jakościowych i liczbowych, a y może być kategoryczne dla klasyfikacji lub numeryczne dla regresji \cite{Breiman1984}.\par
\vskip 0.2in
Największą zaletą drzew decyzyjnych jest to, że mogą modelować dowolny typ funkcji do klasyfikacji lub regresji, czego inne techniki nie potrafią. Ponadto drzewa decyzyjne są uważane za metodę nieparametryczną, co oznacza nie mają one żadnych założeń co do rozkładu danych i struktury klasyfikatora. Drzewa decyzyjne zapewniają również znacznie szybsze trenowanie w porównaniu z prostymi sieciami neuronowymi przy porównywalnej wydajności (złożoność czasowa drzew decyzyjnych jest funkcją zależną od liczby cech oraz wierszy w zestawie danych, podczas gdy dla sieci neuronowych jest funkcją zależną od liczby cech, wierszy w zestawie danych, warstw ukrytych oraz węzłów w każdej ukrytej warstwie). Wadą drzew decyzyjnych jest podatność na przeuczenie. Zatem drzewa decyzyjne są używane najczęściej w przypadku bardzo dużych zbiorów danych, które są uważane za dobrze reprezentujące rzeczywistość. Niektóre algorytmy przycinania drzew są używane do rozwiązania problemu przeuczenia \cite{MazumdarWWW, Breiman1984, AnalyticsWWW, Lan2017}.

\section{Rodzaje kryteriów podziału}
W procecie budowy drzewa decyzyjengo należy wielokrotnie dokonać podziału zbioru danych, tj. należy zadać więcej niż jedno pytanie: co jest cechą, od której powinniśmy zacząć (węzeł główny) i w jakiej kolejności powinniśmy budować węzły wewnętrzne, to znaczy używać opisowych cech, aby podzielić zbiór danych? W związku z tym przydatne byłoby zmierzenie "informatywności" funkcji i wykorzystanie tej funkcji z największą "informacyjnością" jako cechą, która powinna być używana do dzielenia danych. 

\subsection{Współczynnik Giniego}
Współczynnik Giniego lub indeks Giniego to miara koncentracji (nierównomierności) rozkładu zmiennej losowej. Nazwa współczynnika pochodzi od nazwiska jego twórcy, włoskiego statystyka Corrado Giniego. Jest on wykorzystywany przez algorytm CART do mierzenia tego, jak często losowo wybrany element z zestawu byłby niewłaściwie oznaczany, gdyby był losowo oznaczony zgodnie z rozkładem etykiet w podzbiorze. Gini index można obliczyć, sumując prawdopodobieństwo $p_{i}$ elementu o etykiecie $i$ wybieranej razy prawdopodobieństwo 
$\sum_{k \neq i} p_{k} = 1-p_{i}$ o pomyłce w kategoryzacji tego przedmiotu. Osiąga minimum (zero), gdy wszystkie przypadki w węźle należą do jednej kategorii docelowe \cite{AnalyticsWWW, AIspaceWWW}.

\subsection{Information gain}
Information gain (wzmocnienie informacji) - atrybutem podziału jest ten, który ma maksymalny przyrost informacji. Aby móc obliczyć przyrost informacji, konieczne jest wprowadzenie terminu entropii zbioru danych. Entropia zbioru danych służy do pomiaru nierównomierności zbioru danych, a w przypadku drzew decyzyjnych jest używana jako miernik informatywności. Termin entropia (w teorii informacji) pochodzi od Claude'a E. Shannona. Idea entropii jest w uproszczeniu następująca: wyobraź sobie, że masz pudełko, które zawiera 100 białych kulek. Zestaw kulek w pudełku można uznać za całkowicie równomierny, ponieważ zawieraja tylko białe kulki (zbiór kulek ma entropię 0, tj. zero nierównomierności). Jeżeli 30 z tych kulek zostało by zastąpionych przez szare, a 20 przez czarne, to gdyby teraz zechcieć wyjąć jedną kulkę z pudełka to prawdopodobieństwo otrzymania białej kulki spadło by z 1,0 do 0,5. Oznacza to, że nierównomierność wzrosła, równomierność zmniejszyła się, a entropia wzrosła. Podsumowując im bardziej nierównomierny jest zbiór danych, tym wyższa entropia, a im mniej nierównomierny zbiór danych, tym niższa entropia \cite{MazumdarWWW, AnalyticsWWW}

\subsection{Gain ratio}
Gain ratio (współczynnik wzmocnienia) - wybiera atrybut o najwyższym wzroście informacji do liczby współczynników wartości wejściowych. Liczba wartości wejściowych to liczba odrębnych wartości atrybutu występującego w zbiorze treningowym. Jest on stosunkiem przyrostu informacji do wewnętrznej informacji. Jest stosowany w celu zmniejszenia błędu w kierunku atrybutów o wielu wartościach, biorąc pod uwagę liczbę i rozmiar oddziałów przy wyborze atrybutu \cite{AnalyticsWWW, AIspaceWWW, Brownlee2017}.

\section{Algorytmy budowania drzew decyzyjnych}
Dostępnych jest kilka algorytmów służących do klasyfikacji i analizy segmentacji. Wszystkie te algorytmy zasadniczo realizują to samo zadanie: dzieląc dane na kolejne podgrupy, analizują wszystkie zmienne w zbiorze danych, by znaleźć zmienną zapewniającą najlepszą klasyfikację lub predykcję. Proces jest rekursywny, a grupy są dzielone na coraz mniejsze jednostki aż do ukończenia drzewa (zgodnie z określonym kryterium zatrzymania). Popularne algorytmy dzielenia obejmują minimalizację Gini Impurity (używanego przez CART) lub maksymalizację Information Gain (używanego przez ID3, C4.5) \cite{ WhatWhenWWW}.\par
\vskip 0.2in
Przegląd istniejącej literatury pokazuje, że do najczęściej stosowanych algorytmów drzewa decyzyjnego należą algorytm Iterative Dichotomiser 3 (ID3), algorytm C4.5, CHAID oraz CART algorytm. Wśród tych algorytmów są pewne różnice, z których jedną jest możliwość modelowania różnych typów danych. Ponieważ zestaw danych może być skonstruowany na podstawie różnych typów danych, np. danych kategorycznych, danych numerycznych lub kombinacji obu, istnieje potrzeba użycia odpowiedniego algorytmu drzewa decyzyjnego, który może obsługiwać określony typ danych wykorzystywanych w zestawie danych. Wszystkie wyżej wymienione algorytmy mogą wspierać modelowanie danych jakościowych, podczas gdy tylko algorytm C4.5, C5.0 i algorytm CART mogą być używane do modelowania danych numerycznych (Tab.~\ref{decision_algo}). Inną różnicą między tymi algorytmami jest proces opracowywania modeli, szczególnie na etapie budowania i przycinania drzew. Algorytmy ID3, C4.5 i C5.0 dzielą model drzewa tyle rozgałęzień ile można osiągnąć w danym momencie, podczas gdy algorytm CART obsługuje jedynie binarne podziały. Z kolei mechanizmy przycinania zlokalizowane w algorytmach C4.5, C5.0 i CART wspierają usuwanie nieistotnych węzłów i rozgałęzień. Natomiast algorytm CHAID nie wymaga dodatkowego etapu przycinania drzewa, z uwagi na fakt że CHAID stara się zapobiegać przeuczeniu od samego początku, wykorzystując pomysł wstępnego przycinania (węzeł jest dzielony tylko wtedy, gdy spełnione jest kryterium istotności) \cite{AnalyticsWWW, WhatWhenWWW}.

\begin{table}[ht]
\centering
\caption{Porówanie popularnych algorytmów budowania drzew decyzyjnych}
\label{decision_algo}
\begin{tabular}{@{}lll@{}}
\toprule
\textbf{Algorytm}                    & \textbf{Typ danych}      & \textbf{Metoda podziału danych numerycznych} \\ \midrule
CHAID \cite{Kass1980}                & Kategoryczne             & Nie dotyczy                                  \\ \midrule
ID3 \cite{Quinlan1986}               & Kategoryczne             & Nie dotyczy                                  \\ \midrule
C4.5 \cite{Quinlan1993}              & Kategoryczne, numeryczne & Brak ograniczeń                              \\ \midrule
C5.0                                 & Kategoryczne, numeryczne & Brak ograniczeń                              \\ \midrule
CART \cite{Breiman1984}              & Kategoryczne, numeryczne & Podziały binarne                             \\ \bottomrule
\end{tabular}
\end{table}

\subsection{Algorytm CART}
\label{sub:CART}
Algorytm CART (ang. Classification and Regression Trees) jest popularnym algorytmem uczenia drzew decyzyjnych. W odróżnieniu od ID3 i C4.5, drzewo uczenia się w tym przypadku może być używane zarówno do klasyfikacji wieloklasowej, jak i do regresji w zależności od rodzaju zmiennej zależnej. Proces budowy drzewa składa się z rekurencyjnego dwójkowego podziału węzłów. Aby znaleźć najlepszy podział w każdym węźle, rozważane są wszystkie możliwe podziały wszystkich dostępnych atrybutów predykcyjnych. Najlepszy podział to taki, który maksymalizuje pewne kryterium podziału. Do zadań klasyfikacyjnych, tj. gdy atrybut zależny jest kategoryczny, jako kryterium podziału stosuje się indeks Giniego. Do zadań regresyjnych, tj. gdy zmienna zależna jest ciągła, stosowane jest metoda najmniejszych kwadratów \cite{MazumdarWWW, Breiman1984, Brownlee2017}.

\subsection{Algorytm CHAID}
CHAID to algorytm do uczenia drzew decyzyjnych zaproponowany przez Kassa (1980). Działa podobnie do CART - oba mogą być używane zarówno do klasyfikacji, jak i regresji. Ale w przeciwieństwie do CART, CHAID wewnętrznie obsługuje tylko kategoryczne funkcje. Funkcje ciągłe są najpierw konwertowane na zmienne kategoryczne za pomocą grupowania/kubełkowania (ang. binning). Liczba kubełków (ang. bins) (K) musi być dostarczona przez użytkownika. Biorąc pod uwagę K, predykator jest podzielony w taki sposób, że wszystkie kubełki mają mniej więcej taką samą liczbę różnych wartości predykcyjnych. Maksymalna wartość funkcji w każdym pojemniku jest używana jako punkt przerwania \cite{MazumdarWWW, Kass1980}.\par
\vskip 0.2in
Ważnym parametrem w procesie wzrostu drzewa CHAID jest wartość p. Wartość p jest miarą używaną do decydowania o tym, które kategorie wartości predykcyjnych mają się łączyć podczas łączenia, a także do decydowania o najlepszym atrybucie podczas dzielenia. Wartość p jest obliczana przy użyciu różnych metod testowania hipotez w zależności od rodzaju zmiennej zależnej (nominalnej, porządkowej lub ciągłej) \cite{MazumdarWWW}.

\subsection{Algorytm ID3}
ID3 jest prostym algorytmem uczenia drzewa decyzyjnego opracowanym przez Quinlana (1986). ID3 ma zastosowanie tylko w przypadkach, w których atrybuty (lub cechy) definiujące przykłady danych mają charakter kategoryczny, a przykłady danych należą do wcześniej zdefiniowanych, wyraźnie odróżnialnych (tj. dobrze zdefiniowanych) klas. ID3 to iteracyjny chciwy algorytm, który rozpoczyna się od węzła głównego i ostatecznie buduje całe drzewo. W każdym węźle wybiera się "najlepszy" atrybut do klasyfikacji danych. Atrybut "najlepszy" jest wybierany przy użyciu metryki Information gain. Po wybraniu atrybutu w węźle, przykłady danych w węźle są podzielone na podgrupy na podstawie wartości atrybutów, które mają. Zasadniczo wszystkie przykłady danych o tej samej wartości atrybutu są umieszczane w tej samej podgrupie. Podgrupy tworzą dzieci obecnego węzła, a algorytm jest powtarzany dla każdego z nowoutworzonych węzłów potomnych. Trwa to dopóki wszystkie elementy danych węzła nie należą do tej samej klasy lub wszystkie atrybuty zostaną wyczerpane \cite{MazumdarWWW, Quinlan1986}.

\subsection{Algorytm C4.5}
Algorytm C4.5 jest rozszerzeniem algorytmu ID3. Ma dodatkową możliwość obsługi ciągłych atrybutów i atrybutów z brakującymi wartościami. Proces budowy drzew w przypadku C4.5 jest taki sam jak w przypadku ID3 - znajdowanie najlepszego podziału w każdym węźle przy użyciu metryki Information gain. Jednak w przypadku atrybutu ciągłego algorytm C4.5 musi wykonać dodatkowy krok przekształcania go w dwuwartościowy atrybut kategoryczny, dzieląc około odpowiedniego progu. Próg ten jest wybierany w taki sposób, że wynikowy podział daje maksymalne wzmocnienie informacji \cite{MazumdarWWW, Quinlan1993}.

\subsection{Algorytm C5.0}
C5.0 to najnowsza wersja Quinlana na podstawie licencji firmowej. Wykorzystuje mniej pamięci i buduje mniejsze zestawy reguł niż C4.5, a jednocześnie jest bardziej dokładna \cite{scikit}.

\section{Przycinanie drzew decyzyjnych}
Wydajność drzewa można dodatkowo zwiększyć przez przycinanie. Polega ono na usunięciu gałęzi, które korzystają z funkcji o niskim znaczeniu. W ten sposób zmniejszamy złożoność drzewa, a tym samym zwiększamy jego moc predykcyjną, zmniejszając przeuczenie. Przycinanie może rozpocząć się od korzenia lub liści. Najprostsza metoda przycinania rozpoczyna się na liściach i usuwa każdy węzeł z najbardziej popularną klasą w tym liściu, zmiana ta jest zachowana, jeśli nie pogarsza dokładności. Bardziej zaawansowaną metodą przycinania, jest przycinanie kosztów, gdy parametr uczenia (alfa) jest używany do ważenia, czy węzły mogą być usunięte w oparciu o rozmiar pod-drzewa. \cite{Breiman1984, Lan2017}.


\section{Biblioteka scikit-learn}
\subsection{Wstęp}
Scikit-learn to bezpłatna biblioteka do uczenia maszynowego z wykorzytaniem języka. Zawiera ona różne algorytmy klasyfikacji, regresji i grupowania, w tym metodę drzew decyzyjnych. Co więcej, bibliteka ta jest przystosowana współdziałania z numerycznymi i naukowymi bibliotekami NumPy i SciPy w Pythonie. Popularne grupy modeli dostarczane przez scikit-learn to:
\begin{itemize}
	\item Clustering: do grupowania nieoznakowanych danych, takich jak k-means.
	\item Cross Validation: do oceny wydajności nadzorowanych modeli na niewidocznych danych.
	\item Datasets: dla zestawów danych testowych i do generowania zestawów danych o określonych właściwościach do badania zachowania modelu.
	\item Dimensionality Reduction: w celu zmniejszenia liczby atrybutów w danych do podsumowania, wizualizacji i wyboru funkcji, takich jak analiza głównych składników.
	\item Ensemble methods: do łączenia przewidywań wielu nadzorowanych modeli.
	\item Feature extraction: do definiowania atrybutów w obrazie i danych tekstowych.
	\item Feature selection: do identyfikacji znaczących atrybutów, z których można tworzyć nadzorowane modele.
	\item Parameter Tuning: aby uzyskać jak najwięcej z nadzorowanych modeli.
	\item Manifold Learning: do podsumowywania i przedstawiania złożonych wielowymiarowych danych.
	\item Supervised Models: modele liniowe, analiza dyskryminacyjna, sieci neuronowe, drzewa decyzyjne i wiele więcej.
\end{itemize}

\subsection{Moduł drzew decyzyjnych}
Moduł drzew decyzyjnych - sklearn.tree zawiera oparte na drzewach decyzyjnych modele klasyfikacji i regresji. Klasy zawarte w tym module to:
\begin{itemize}
	\item tree.DecisionTreeClassifier ([criterion, ...]) - Klasyfikator drzewa decyzyjnego.
	\item tree.DecisionTreeRegressor ([criterion, ...]) - Regressor drzewa decyzyjnego.
	\item tree.ExtraTreeClassifier ([criterion, ...]) - Niezwykle losowy klasyfikator drzewa.
	\item tree.ExtraTreeRegressor ([criterion, ...]) - Niezwykle losowy regressor drzewa.
	\item tree.export\_graphviz (decision\_tree , [...]) - Eksportowanie drzewa decyzyjngo w formacie DOT.
\end{itemize}
Najważnijesze z powyższych klas opisano w kolejnych podrozdziałach \ref{subsub:DecisionTreeClassifier} oraz \ref{subsub:DecisionTreeRegressor}.

\subsubsection{Klasa DecisionTreeClassifier}
\label{subsub:DecisionTreeClassifier}
\begin{flushleft}
	\textit{class sklearn.tree.DecisionTreeClassifier(criterion=’gini’, splitter=’best’, max\_depth=None, min\_samples\_split=2, min\_samples\_leaf=1, min\_weight\_fraction\_leaf=0.0, max\_features=None, random\_state=None, max\_leaf\_nodes=None, min\_impurity\_decrease=0.0, min\_impurity\_split=None, class\_weight=None, presort=False)}
\end{flushleft}\par
\vskip 0.2in

DecisionTreeClassifier() to funkcja klasyfikująca dla DecisionTree. Stanowi ona konstruktor klasy DecisionTreeClassifier. Ważne parametry to:
\begin{itemize}
	\item \textbf{criterion:} definiuje funkcję pomiaru jakości podziału. Sklearn obsługuje kryteria "gini" dla indeksu Gini i "entropy" dla Information Gain. Domyślnie przyjmuje wartość "gini".
	\item \textbf{splitter:} definiuje strategię wyboru podziału w każdym węźle. Wartość "best" oznacza, że należy wybrać najlepszy podział, a "random", aby wybrać najlepszy podział losowy. Domyślnie przyjmuje wartość "best".
	\item \textbf{max\_features:} definiuje ilość funkcji do rozważenia przy poszukiwaniu najlepszego podziału. Można wprowadzić liczbę całkowitą, zmiennoprzecinkową, łańcuchową i wartość None.
	\begin{itemize}
		\item Jeśli wprowadzona jest liczba całkowita jest ona brana za maksymalną wartością dla każdego podziału.
		\item Jeśli zostanie użyta wartość zmiennoprzecinkowa, pokazuje ona procent elementów w każdym podziale.
		\item Jeśli zostanie wybrane "auto" lub "sqrt", wówczas max\_features = $\sqrt{n\_features}$.
		\item Jeśli zostanie wybrany log2, wówczas max\_features = $\log_2{(n\_features)}$.
		\item Jeśli nie wprowadzono własnej wartości (wartość domyślna None), wówczas max\_features = n\_features.
	\end{itemize}

	\item \textbf{max\_depth:} parametr określa maksymalną głębokość drzewa. Może przyjmować dowolną liczbę całkowitą lub None. Jeśli ustawiono None, to węzły są rozszerzane, aż wszystkie próbki zostaną wykorzystane lub dopóki wszystkie liście nie zawierają próbek mniejszych niż min\_samples\_split. Domyślnie przyjmuje wartość None.
	\item \textbf{min\_samples\_split:} parametr określa minimlaną liczba próbek do podziąłu. Jeśli zostanie podana wartość całkowita, należy rozważyć min\_samples\_split jako wartość mininalną, a jeśli podano liczbę zmiennoprzecinkowoą, wówczas pokazuje ona procent próbek. Domyślnie przyjmuje wartość 2.
	\item \textbf{min\_samples\_leaf:} minimalna liczba próbek, które muszą znajdować się w węźle liści. Jeśli zostanie podana wartość całkowita, należy rozważyć min\_samples\_leaf jako wartość minimalną, a jeśli zmiennoprzecinkowa, to pokazuje ona procent próbek. Domyślnie przyjmuje wartość 1.
	\item \textbf{max\_leaf\_nodes:} określa maksymalną liczbę możliwych liści. Jeśli None, pobiera nieograniczoną liczbę węzłów liści. Domyślnie przyjmuje wartość Brak.
	\item \textbf{min\_impurity\_split:} definiuje próg wczesnego zatrzymywania wzrostu drzewa. Węzeł zostanie podzielony, jeśli jego nierównomierność przekroczy próg, w przeciwnym razie jest to liść.
\end{itemize}
\vskip 0.2in

Metody klasy DecisionTreeClassifier to:
\begin{itemize}
	\item \textbf{apply(X [, check\_input])} - zwraca indeks liścia, dla którego każda próbka jest przewidywana.
	\item \textbf{decision\_path(X [, check\_input])} - zwraca ścieżkę decyzyjną w drzewie
	\item \textbf{fit(X, y [, sample\_weight, check\_input, ...])} - zbuduj klasyfikator drzewa decyzyjnego z zestawu treningowego (X, y).
	\item \textbf{get\_params([deep])} - pobierz parametry tego estymatora.
	\item \textbf{predict(X [, check\_input])} - wyznacz klasę lub wartość regresji dla X.
	\item \textbf{predict\_log\_proba(X)} - przewiduj log-probabilitie klasy o X próbkach wejściowych.
	\item \textbf{predict\_proba(X [, check\_input])} - przewiduj prawdopodobieństwo klasowe próbek wejściowych X.
	\item \textbf{score(X, y [, sample\_weight])} - zwraca średnią dokładność podanych danych testowych i etykiet.
	\item \textbf{set\_params(** params)} - ustaw parametry tego estymatora.
\end{itemize}

\subsubsection{Klasa DecisionTreeRegressor}
\label{subsub:DecisionTreeRegressor}
\begin{flushleft}
	\textit{class sklearn.tree.DecisionTreeRegressor(criterion=’mse’, splitter=’best’, max\_depth=None, min\_samples\_split=2, min\_samples\_leaf=1, min\_weight\_fraction\_leaf=0.0, max\_features=None, random\_state=None, max\_leaf\_nodes=None, min\_impurity\_decrease=0.0, min\_impurity\_split=None, presort=False)}
\end{flushleft}\par
\vskip 0.2in


\section{Opis programu}

\section{Wnioski}

\section{Opisanie danych wykorzystanych do testowania działania metody wybranej biblioteki}

\section{Omówienie głównych części programu/skryptu i wyników}

\section{Wnioski końcowe odnośnie biblioteki i projektu}

\Urlmuskip=0mu plus 1mu\relax
\addcontentsline{toc}{section}{Literatura}
\bibliography{Projekt}

\end{document}
